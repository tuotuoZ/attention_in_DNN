<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c0{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:left;height:11pt}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c13{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c4{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:left}.c18{padding-top:0pt;padding-bottom:3pt;line-height:2.0;page-break-after:avoid;text-align:left}.c20{text-decoration-skip-ink:none;font-size:13pt;-webkit-text-decoration-skip:none;color:#660099;text-decoration:underline}.c8{padding-top:18pt;padding-bottom:6pt;line-height:2.0;page-break-after:avoid;text-align:left}.c2{-webkit-text-decoration-skip:none;color:#0b0080;text-decoration:underline;text-decoration-skip-ink:none}.c16{background-color:#ffffff;font-size:10.5pt;color:#222222}.c9{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c3{color:inherit;text-decoration:inherit}.c15{width:33%;height:1px}.c1{background-color:#ffffff;font-size:9pt}.c14{font-size:12pt}.c10{font-style:italic}.c17{font-weight:700}.c11{font-size:10pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c9"><p class="c18 title" id="h.6j9k6lubyfvz"><span class="c19">Background Check(draft)</span></p><p class="c0"><span class="c12"></span></p><h2 class="c8" id="h.8uoekfivoh4m"><span class="c7">Intro</span></h2><p class="c4"><span class="c14">Neural Network becomes one of the most popular topic during the past few years. There is no doubt that Neural Network is one of the most powerful tool for classification problem. A model called Alexnet took us to the new state of art at Image Competition 2012. It outperformed the second best model by 10.8 percent in the classification task. </span><sup class="c14"><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c5">The massive improvement in accuracy was because of applying a new technique called Convolutional Neural Networks. </span></p><h2 class="c8" id="h.niv2dceyphk5"><span class="c7">Origin of CNN</span></h2><p class="c4"><span>Using Convolutional Neural Networks on image classification problems was inspired by the biological process in animal visual cortex.</span><sup><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span>&nbsp;The discovery of this biological process can be traced back to 1950s. Hubel and Weisel revealed the receptive field in monkey&rsquo;s brain. After that, LeCun used back-propegation to get a model with trainalbe weights to classify hand-written zip code numbers in 1989.</span><sup><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span>&nbsp;The learning process got </span><span class="c16">convolution kernel coefficients automatically and beated the manual coefficient design.</span><span>&nbsp;</span><span>Then, LeCun improved the model and used a 7 layer called LeNet-5 which could take higher resolution image than before, but this method was also constrained by the limited computation power at that time.</span><sup><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup></p><p class="c0"><span class="c12"></span></p><h2 class="c8" id="h.3h8ykeo8o7ox"><span class="c7">My research direction</span></h2><p class="c4"><span>What draws my attention in this field is a study blog from Dan Vatterott. Usually, CNN has three types of layers, input layer, hidden layer and output layer, and it has at least each of the layer to be a complete CNN. Dan put an extra feature in the CNN which is visual attention and the prediction accuracy in a more noised handwritten digit image set.</span><sup><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup><span class="c12">&nbsp;It showed the potential that introducing attention in CNN model can make the whole system more robust and accurate. Dan chose the MNIST which is relatively easy task because the best model before can predict accurately over 99%. My goal is to test the attention feature in more complicated task, and show the significance on improving the robustness for a model.</span></p><p class="c0"><span class="c12"></span></p><p class="c4"><span>If my plan goes well, I will also extend the attention feature in a video recognition model to see if it&rsquo;s also a better solution for video motion processing. Luckily, a paper from Universit &#769;e de Montr &#769;eal,yUniversity of Toronto demonstrated great improvement on caption generation using attenion in DNN. They used attention to give different position of the image determining which position is more important. This is important because caption generation involves express the image in English, a natural language. For example, if you see a picture like this, a non-attentional model would give you two simple words: bird and water. A more sophisticated answer would be: A bird flying over a body of water. And that require a lot more abstraction than just water and bird.</span><sup><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup><span>&nbsp;The model has to understand the center of the picture is the bird, the water is the background, and the motion of the motion of the bird is flying. Surely, it cannot be done by a single attention image recognition DNN to generate the whole caption, and they actually combining the image CNN with a natural language processing CNN. Attention surely played a important role in this study. They also used LSTM which I am interested in and I am planning to learn the LSTM technique in my video processing.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 186.00px; height: 180.00px;"><img alt="" src="images/image1.png" style="width: 186.00px; height: 180.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c8" id="h.zdihjf8erupm"><span class="c7">Speed up part</span></h2><p class="c4"><span class="c5">Besides the prediction accuracy, the design team of Alexnet implemented the model in GPUs, which took them 6 days to train the model on two GTX 580 3GB. It was considered quite feasible to achieve at that time, and they also explained the limitation of the model was due to the computation power and memory on the graphic card. Today&rsquo;s latest GTX 2080 8GB has nearly 4 times computation power and 3 times available memory so you can get the model trained within 2 days. Moreover, cloud computing services, like AWS or Google Cloud Platform, have become more and more accessible to developers. It&rsquo;s easy to set up a server with a bunch of those high-end GPUs and speed up the training process even more.</span></p><p class="c0"><span class="c5"></span></p><h2 class="c8" id="h.7rnwhy6izaxe"><span class="c7">Plan</span></h2><p class="c4"><span class="c12">I am taking a machine learning class and Smith College at the 2018 Fall semester. This course will cover Neural Network techniques at the end of the semester. Besides that, I also found a great course in Udemy teaching specifically Deep Neural Network in Python. At the preview of this course, the instructor emphasized the importance of the being familiar with different regression model. The reason is that those statistical tools we are using in the regression model help us understand the DNN better. Fortunatelly, the machine learning class at Smith mainly covers the basic regression models and stastical part. I will use these two courses as my main direction to my research.</span></p><p class="c0"><span class="c5"></span></p><p class="c0"><span class="c5"></span></p><hr class="c15"><div><p class="c13"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c6">&nbsp;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</span></p></div><div><p class="c13"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c6">&nbsp;https://www.sciencedirect.com/science/article/pii/S0893608003001151?via%3Dihub</span></p></div><div><p class="c13"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c11">&nbsp;</span><span class="c1">Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel, </span><span class="c2 c1"><a class="c3" href="https://www.google.com/url?q=http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf&amp;sa=D&amp;ust=1540522361554000">Backpropagation Applied to Handwritten Zip Code Recognition</a></span><span class="c1">; AT&amp;T Bell Laboratories</span></p></div><div><p class="c13"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c11">&nbsp;</span><span class="c1">LeCun, Yann; L&eacute;on Bottou; Yoshua Bengio; Patrick Haffner (1998). </span><span class="c1 c2"><a class="c3" href="https://www.google.com/url?q=http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf&amp;sa=D&amp;ust=1540522361554000">&quot;Gradient-based learning applied to document recognition&quot;</a></span><span class="c1">&nbsp;(PDF). </span><span class="c1 c10">Proceedings of the IEEE</span><span class="c1">. </span><span class="c1 c17">86</span><span class="c1">&nbsp;(11): 2278&ndash;2324. </span><span class="c2 c1"><a class="c3" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Digital_object_identifier&amp;sa=D&amp;ust=1540522361554000">doi</a></span><span class="c1">:</span><span class="c2 c1"><a class="c3" href="https://www.google.com/url?q=https://doi.org/10.1109%252F5.726791&amp;sa=D&amp;ust=1540522361555000">10.1109/5.726791</a></span><span class="c1">. Retrieved October 7, 2016.</span></p></div><div><p class="c13"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c6">&nbsp;https://danvatterott.com/blog/2016/09/20/attention-in-a-convolutional-neural-net/</span></p></div><div><p class="c13"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c11">&nbsp;</span><span class="c20"><a class="c3" href="https://www.google.com/url?q=http://www.jmlr.org/proceedings/papers/v37/xuc15.pdf&amp;sa=D&amp;ust=1540522361555000">Show, attend and tell: </a></span><span class="c20 c17"><a class="c3" href="https://www.google.com/url?q=http://www.jmlr.org/proceedings/papers/v37/xuc15.pdf&amp;sa=D&amp;ust=1540522361556000">Neural </a></span><span class="c20"><a class="c3" href="https://www.google.com/url?q=http://www.jmlr.org/proceedings/papers/v37/xuc15.pdf&amp;sa=D&amp;ust=1540522361556000">image caption generation with visual </a></span><span class="c20 c17"><a class="c3" href="https://www.google.com/url?q=http://www.jmlr.org/proceedings/papers/v37/xuc15.pdf&amp;sa=D&amp;ust=1540522361556000">attention</a></span></p></div></body></html>